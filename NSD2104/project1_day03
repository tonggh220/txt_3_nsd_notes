PROJECT1_DAY03 ,要完成的实操任务
一 案例1：Keepalived高可用（解决调度器haproxy的单点故障问题）
	准备1台新的虚拟机，在eth1网卡配置IP地址192.168.2.6
					    在eth0网卡配置IP地址192.168.4.6
	
	1.1 部署HAProxy ： 安装软件  修改配置文件  启动haporxy 服务
	
	1.2 分别在两台代理服务器配置keepalived
			1 分别安装keepalived软件
			2 分别修改2台主机的主配置文件
			3 分别2台主机的启动keepalived服务
			
	1.3 测试高可用集群    
			1  在优先级高的 主机上查看vip地址  查看到是对的
[root@haproxy5 ~]# ip addr show  | grep  192.168.4.80
inet 192.168.4.80/32 scope global eth0
	
			2 停止优先级高的 主机上keepalived服务 模拟主机故障
[root@haproxy5 ~]# systemctl  stop keepalived
			
			3 在优先级低的 主机上查看vip地址  查看到是对的
[root@host6 ~]# ip  a  s  | grep 192.168.4.80
    inet 192.168.4.80/32 scope global eth0
			
			4 启动优先级高的 主机上keepalived服务 后 ， 会抢占vip地址
[root@haproxy5 ~]# systemctl  start keepalived			
[root@haproxy5 ~]# ip a s  | grep  192.168.4.80
    inet 192.168.4.80/32 scope global eth0
			
			5 客户端连接VIP地址192.168.4.80 访问网站服务 可以正常看的页面是对的
[root@localhost ~]# curl  http://192.168.4.80/testa.php
web12

	1.4 修改DNS的配置
		说明：配置了高可用集群依然让客户端可以通过www.lab.com 访问我们的网站服务
		1 修改192.168.4.5主机的dns服务的解析文件
~]# vim /var/named/lab.com.zone 	    
;www    A       192.168.4.5
www     A       192.168.4.80
]# systemctl  restart named
		
		2 客户端测试
[root@localhost ~]# cat /etc/resolv.conf 
# Generated by NetworkManager
#nameserver 219.141.136.10
#nameserver 219.141.140.10
#nameserver 172.40.92.6
nameserver 192.168.4.5
[root@localhost ~]# curl  http://www.lab.com/testa.php
web12[		
	
			
二 案例2：部署Ceph分布式存储（解共享网页NFS服务的单点故障问题）
	环境准备：克隆3台新的虚拟机，IP地址分别为
			  eth1  192.168.2.41   node1 （即做管理主机又是集群成员主机）
			  eth1  192.168.2.42   node2
			  eth1  192.168.2.43   node3
			  
			  每台虚拟各自添加2块 20G 的硬盘
			  
			  
	搭建ceph集群的环境：具体配置如下：
			1 在node1配置SSH密钥，让node1可用无密码连接node1,node2,node3 
			（配置管理主机node1 可以ssh免密登录所有集群成员主机）
			
			2 修改/etc/hosts域名解析记录

			3 配置安装ceph软件的本地yum源

			4 配置NTP服务器同步时间。（使用node1 充当ntp服务器）
			  4.1  配置NTP服务	  
[root@node1 ~]# vim /etc/chrony.conf
 28 #allow 192.168.0.0/16
 29 allow 192.168.2.0/24
 30 # Serve time even if not synchronized to a time source.
 31 local stratum 10
:wq
[root@node1 ~]# systemctl restart chronyd
			  
			  4.2  配置客户端 （node2 和 node3 ）
]# vim /etc/chrony.conf			  
			   #server gateway iburst
server 192.168.2.41 iburst
:wq
]# systemctl restart chronyd
]# chronyc sources -v			   
^* node1   ......
               
		部署ceph集群,具体步骤如下：
				1  给node1主机安装ceph-deploy，创建工作目录，初始化配置文件。
				
				2  给所有ceph节点安装ceph相关软件包（在node1主机执行命令）
				
				3 初始化mon服务（在node1主机执行命令）
				

出错的终极解决办法 ：
	node1]# ceph-deploy purge node1  node2  node3   #卸载3台主机安装的ceph软件
	node1]# ceph-deploy purgedata  node1  node2  node3   #删除3台主机的数据
    #检查集群环境准备	
	#再次在管理主机执行： 部署ceph集群			
				
				4 使用ceph-deploy工具初始化数据磁盘（在node1主机执行命令）
				5 初始化OSD集群，磁盘名称根据实际情况填写。（在node1主机执行命令）
				
				部署ceph文件系统
						1  启动mds服务
						2  创建存储池 cephfs_metadata   cephfs_data 
						3  创建文件系统 myfs1
						
			
				迁移网站数据到ceph集群:
					休息到17：10 
						1  停止3台网站服务器的nginx服务
					
						2  在3台网站服务器 卸载对nfs服务的当前挂载和永久挂载
						
						3  在3台网站服务器分别挂载 ceph共享    
                        注意：有3种挂载方式 ，使用任意就可以（统一使用rc.local）
注意 在集群的任意1台服务器查看连接用户名和密码
						统一在node1主机查看
[root@node1 ceph-cluster]# cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
        key = AQB8pgNhhDTeJxAAw6u3CAw9g+PHwQKU6nXcsQ==
[root@node1 ceph-cluster]#	

						4 启动3台网站服务器的nginx服务
						 
				迁移NFS服务器中的数据到Ceph存储
						1登陆NFS服务器备份数据，将备份数据拷贝给web1或web2或web3 中的
						任意一台都可以 （统一拷贝给web1）
						
						2 登陆web1将数据恢复到Ceph共享目录
				
					    3  访问wordpress 
						打开浏览器 输入网址 http://www.lab.com
